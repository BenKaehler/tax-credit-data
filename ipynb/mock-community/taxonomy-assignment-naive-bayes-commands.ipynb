{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import join\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "from qiime2 import Artifact\n",
    "from qiime2.plugins import feature_classifier\n",
    "from q2_types.feature_data import DNAIterator\n",
    "from q2_feature_classifier.classifier import \\\n",
    "    spec_from_pipeline, pipeline_from_spec, _register_fitter\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from tax_credit.framework_functions import \\\n",
    "    generate_per_method_biom_tables, move_results_to_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Paths and Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_dir = join('..', '..')\n",
    "analysis_name = 'mock-community'\n",
    "data_dir = join(project_dir, 'data', analysis_name)\n",
    "precomputed_dir = join(project_dir, 'data', 'precomputed-results', analysis_name)\n",
    "\n",
    "ref_db_dir = join(project_dir, 'data', 'ref_dbs')\n",
    "\n",
    "gg_db = join(ref_db_dir, 'gg_13_8_otus/99_otus.fasta')\n",
    "gg_tax = join(ref_db_dir, 'gg_13_8_otus/99_otu_taxonomy.txt')\n",
    "unite_db = join(ref_db_dir, 'unite_20.11.2016/sh_refs_qiime_ver7_99_20.11.2016_dev_clean.fasta')\n",
    "unite_tax = join(ref_db_dir, 'unite_20.11.2016/sh_taxonomy_qiime_ver7_99_20.11.2016_dev_clean.tsv')\n",
    "\n",
    "results_dir = join(project_dir, 'temp_results_narrow')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "mock_dirs = ['mock-' + str(m) for m in \n",
    "             list(range(1, 11)) + list(range(12,17)) +\n",
    "             list(range(18,26)) + ['26-ITS1', '26-ITS9']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reference Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_dest = 'ref_dbs'\n",
    "\n",
    "ref_16S = join(ref_dest, '99_gg_seq.qza')\n",
    "ref_ITS = join(ref_dest, '99_unite_seq.qza')\n",
    "tax_16S = join(ref_dest, '99_gg_tax.qza')\n",
    "tax_ITS = join(ref_dest, '99_unite_tax.qza')\n",
    "\n",
    "if not os.path.exists(join(results_dir, ref_dest)):\n",
    "    os.makedirs(join(results_dir, ref_dest))\n",
    "ref = Artifact.import_data('FeatureData[Sequence]', gg_db)\n",
    "ref.save(join(results_dir, ref_16S))\n",
    "tax = Artifact.import_data('FeatureData[Taxonomy]', gg_tax,\n",
    "                           view_type='HeaderlessTSVTaxonomyFormat')\n",
    "tax.save(join(results_dir, tax_16S))\n",
    "ref = Artifact.import_data('FeatureData[Sequence]', unite_db)\n",
    "ref.save(join(results_dir, ref_ITS))\n",
    "tax = Artifact.import_data('FeatureData[Taxonomy]', unite_tax,\n",
    "                           view_type='HeaderlessTSVTaxonomyFormat')\n",
    "tax.save(join(results_dir, tax_ITS));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplicon Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref_dbs = {}\n",
    "taxs = {}\n",
    "for mock in mock_dirs:\n",
    "    mockdir = join(data_dir, mock)\n",
    "    primer_file = join(mockdir, 'sample-metadata.tsv')\n",
    "    with open(primer_file) as csvfile:\n",
    "        data = next(csv.DictReader(csvfile, delimiter='\\t'))\n",
    "    primers = [data['LinkerPrimerSequence'], data['ReversePrimer']]\n",
    "    sample_type = 'ITS' if 'ITS' in data['PrimerName'] else '16S'\n",
    "    assert sample_type == 'ITS' or '515f' in data['PrimerName']\n",
    "    \n",
    "    if sample_type == '16S':\n",
    "        ref_dbs[mock] = [('gg_13_8_otus_full', ref_16S)]\n",
    "        taxs[mock] = tax_16S\n",
    "        ref = ref_16S\n",
    "        db_name = 'gg_13_8_otus_amplicon'\n",
    "    else:\n",
    "        ref_dbs[mock] = [('unite_20.11.2016_clean_full', ref_ITS)]\n",
    "        taxs[mock] = tax_ITS\n",
    "        ref = ref_ITS\n",
    "        db_name = 'unite_20.11.2016_clean_amplicon'\n",
    "    \n",
    "    if primers[0] == 'CCGTGCCAGCMGCCGCGGTAA':\n",
    "        primers[0] = 'GTGCCAGCMGCCGCGGTAA'\n",
    "    elif primers[0] == 'ATCTTGGTCATTTAGAGGAAGTAA':\n",
    "        primers[0] = 'CTTGGTCATTTAGAGGAAGTAA'\n",
    "    elif 'I' in primers[0]:\n",
    "        primers[0] = primers[0].replace('I', 'N')\n",
    "    \n",
    "    db_file = '_'.join(\n",
    "        [ref.rsplit('.',1)[0]] + \n",
    "        list(primers)) + '.qza'\n",
    "    ref_dbs[mock].append((db_name, db_file))\n",
    "    db_file = join(results_dir, db_file)\n",
    "    if not os.path.exists(db_file):\n",
    "        seqs = Artifact.load(join(results_dir, ref))\n",
    "        trimmed = feature_classifier.methods.extract_reads(\n",
    "                    sequences=seqs, f_primer=primers[0], r_primer=primers[1]).reads\n",
    "        trimmed.save(db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Class Weights\n",
    "\n",
    "map taxonomies to taxonomy labels using `tax-credit/data/mock-community/mock-3/expected-taxonomy.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mock-1\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-2\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-3\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-4\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-5\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-6\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-7\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-8\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-9\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__Kluyveromyces_lactis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Zygosaccharomyces;s__Zygosaccharomyces_rouxii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Hyphopichia;s__Hyphopichia_burtonii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_catenulata\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Cyberlindnera;s__Cyberlindnera_jadinii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Debaryomyces;s__Debaryomyces_hansenii\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Microascales;f__Microascaceae;g__Scopulariopsis\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Trichocomaceae;g__Penicillium;s__Penicillium_commune\n",
      "mock-10\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__Kluyveromyces_lactis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Zygosaccharomyces;s__Zygosaccharomyces_rouxii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Hyphopichia;s__Hyphopichia_burtonii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_catenulata\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Cyberlindnera;s__Cyberlindnera_jadinii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Debaryomyces;s__Debaryomyces_hansenii\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Microascales;f__Microascaceae;g__Scopulariopsis\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Trichocomaceae;g__Penicillium;s__Penicillium_commune\n",
      "mock-12\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-13\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-14\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-15\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-16\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-18\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-19\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-20\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-21\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-22\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-23\n",
      "gg_13_8_otus_full\n",
      "gg_13_8_otus_amplicon\n",
      "mock-24\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Chytridiomycota;c__Chytridiomycetes;o__Spizellomycetales;f__Spizellomycetaceae;g__Spizellomyces;s__Spizellomyces_punctatus\n",
      "mock-25\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Chytridiomycota;c__Chytridiomycetes;o__Spizellomycetales;f__Spizellomycetaceae;g__Spizellomyces;s__Spizellomyces_punctatus\n",
      "mock-26-ITS1\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Cantharellales;f__Hydnaceae;g__Sistotrema;s__Sistotrema_brinkmannii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Debaryomyces;s__Debaryomyces_prosopidis\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Amanitaceae;g__Amanita;s__Amanita_lividopallescens\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Helotiaceae;g__Hymenoscyphus;s__Hymenoscyphus_fraxineus\n",
      "k__Fungi;p__Ascomycota;c__Archaeorhizomycetes;o__Archaeorhizomycetales;f__Archaeorhizomycetaceae;g__Archaeorhizomyces;s__Archaeorhizomyces_finlayi\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_poae\n",
      "k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Rhizinaceae;g__Rhizina;s__Rhizina_undulata\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_purpurascens\n",
      "mock-26-ITS9\n",
      "unite_20.11.2016_clean_full\n",
      "unite_20.11.2016_clean_amplicon\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Helotiaceae;g__Hymenoscyphus;s__Hymenoscyphus_fraxineus\n",
      "k__Fungi;p__Ascomycota;c__Archaeorhizomycetes;o__Archaeorhizomycetales;f__Archaeorhizomycetaceae;g__Archaeorhizomyces;s__Archaeorhizomyces_finlayi\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_poae\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_purpurascens\n"
     ]
    }
   ],
   "source": [
    "weights_dest = 'weights'\n",
    "if not os.path.exists(join(results_dir, weights_dest)):\n",
    "    os.makedirs(join(results_dir, weights_dest))\n",
    "\n",
    "priors_files = {}\n",
    "for mock in mock_dirs:\n",
    "    print(mock)\n",
    "    mockdir = join(data_dir, mock)\n",
    "    for db_name, db_file in ref_dbs[mock]:\n",
    "        print(db_name)\n",
    "        tax_weights = Artifact.load(join(results_dir, taxs[mock]))\n",
    "        seq_ids = Artifact.load(join(results_dir, db_file))\n",
    "        seq_ids = {s.metadata['id'] for s in seq_ids.view(DNAIterator)}\n",
    "        tax_weights = tax_weights.view(Series)\n",
    "        tax_weights = {tax_weights[sid]:0. for sid in tax_weights.index\n",
    "                       if sid in seq_ids}\n",
    "\n",
    "        weights = Artifact.load(join(mockdir, 'feature_table.qza'))\n",
    "        weights = weights.view(DataFrame)\n",
    "        if len(weights.index) > 1:\n",
    "            weights = {s:sum(weights.loc[s]) for s in weights.index}\n",
    "            total = sum(weights.values())\n",
    "            weights = {s:w/total for s, w in weights.items()}\n",
    "        else:\n",
    "            weights = {weights.index[0]: 1.}\n",
    "\n",
    "        et_path = join(precomputed_dir, mock)\n",
    "        if db_name.startswith('gg_13_8_otus'):\n",
    "            et_path = join(et_path, 'gg_13_8_otus')\n",
    "        else:\n",
    "            et_path = join(et_path, 'unite_20.11.2016_clean_fullITS')\n",
    "        et_path = join(et_path, 'expected', 'expected-taxonomy.tsv')\n",
    "        with open(et_path) as tf:\n",
    "            reader = csv.DictReader(tf, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                tax = row['Taxonomy']\n",
    "                weight = sum(weights[s]*float(row[s]) for s in weights)\n",
    "                try:\n",
    "                    tax_weights[tax] += weight\n",
    "                except KeyError:\n",
    "                    species = {t for t in tax_weights if t.startswith(tax)}\n",
    "                    if len(species) == 0:\n",
    "                        print(tax)\n",
    "                    else:\n",
    "                        for s in species:\n",
    "                            tax_weights[s] += weight/len(species)\n",
    "\n",
    "        for tax in tax_weights:\n",
    "            if tax_weights[tax] < 1e-9:\n",
    "                tax_weights[tax] = 1e-9\n",
    "        total = sum(tax_weights.values())\n",
    "\n",
    "        weights = [tax_weights[t]/total for t in sorted(tax_weights)]\n",
    "        filename = mock + '-' + db_name + '-weights.json'\n",
    "        weights_file = join(weights_dest, filename)\n",
    "        priors_files[mock] = priors_files.get(mock, {})\n",
    "        priors_files[mock][db_name] = weights_file\n",
    "        weights_file = join(results_dir, weights_file)\n",
    "        with open(weights_file, 'w') as wf:\n",
    "            json.dump(weights, wf)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Parameter Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_sweep = \\\n",
    "    {'feat-ext--analyzer': ['char'],\n",
    "     'feat-ext--n-features': [8192],\n",
    "     'feat-ext--ngram-range': \n",
    "                     [[4,4], [6,6], [8,8], [16,16], [32,32],\n",
    "                      [7,7], [9,9], [10,10], [11,11], \n",
    "                      [12,12], [14,14], [18,18]],\n",
    "     'classify--alpha': [0.001]}\n",
    "nb_bespoke_sweep = \\\n",
    "    {'feat-ext--analyzer': ['char'],\n",
    "     'feat-ext--n-features': [8192],\n",
    "     'feat-ext--ngram-range': [[4,4], [6,6], [8,8], [16,16], [32,32],\n",
    "                      [7,7], [9,9], [10,10], [11,11], \n",
    "                      [12,12], [14,14], [18,18]],\n",
    "     'classify--alpha': [0.001],\n",
    "     'classify--class-prior': ['prior']}\n",
    "    \n",
    "classifier_params = {'naive-bayes': nb_sweep,\n",
    "                     'naive-bayes-bespoke': nb_bespoke_sweep}\n",
    "\n",
    "confidences = [0.0, 0.5, 0.7, 0.9, 0.92, 0.94,\n",
    "               0.96, 0.98, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier fitting scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classifier_command(method, inputs, params, priors):\n",
    "    cmd = ['qiime feature-classifier fit-classifier-naive-bayes']\n",
    "    cls = [method]\n",
    "    \n",
    "    for param in sorted(inputs):\n",
    "        value = inputs[param]\n",
    "        cmd.extend(['--i-' + param, value])\n",
    "        cls.append(os.path.basename(value).split('.')[0])\n",
    "    \n",
    "    for param in sorted(params):\n",
    "        value = params[param]\n",
    "        if value == 'prior':\n",
    "            cls.append(os.path.basename(priors).split('.')[0])\n",
    "        else:\n",
    "            cls.append(str(value).replace(' ',''))\n",
    "        \n",
    "        if type(value) is bool:\n",
    "            cmd.append('--p-' + ('' if value else 'no-') + param)\n",
    "            continue\n",
    "        \n",
    "        if 'class-prior' not in param:\n",
    "            value = json.dumps(value)\n",
    "            if value[0] != '\"' or value[-1] != '\"':\n",
    "                value = '\"' + value + '\"'\n",
    "            cmd.extend(['--p-' + param, value])\n",
    "            continue\n",
    "            \n",
    "        if value == 'uniform':\n",
    "            continue\n",
    "            \n",
    "        cmd.extend(['--p-' + param, '\"`cat ' + priors + '`\"'])\n",
    "    \n",
    "    cls = ':'.join(cls) + '.qza'\n",
    "    cls = os.path.sep + join('state', 'partition1', 'tmp', 'classifiers_narrow', cls)\n",
    "    \n",
    "    cmd.extend(['--o-classifier', '\"' + cls + '\"'])\n",
    "    cmd = ' '.join(cmd)\n",
    "    \n",
    "    return cls, cmd\n",
    "\n",
    "def get_classify_command(classifier, reads, params, \n",
    "                         confidence, directory, results_dir):\n",
    "    cmd = ['qiime feature-classifier classify-sklearn']\n",
    "    cmd.extend(['--i-classifier', '\"' + classifier + '\"'])\n",
    "    cmd.extend(['--i-reads', reads])\n",
    "    cmd.extend(['--p-confidence', str(confidence)])\n",
    "    \n",
    "    parameters = [str(params[p]).replace(' ', '') for p in sorted(params)]\n",
    "    parameters.append(str(confidence))\n",
    "    output_directory = join(directory, ':'.join(parameters))\n",
    "    if not os.path.exists(join(results_dir, output_directory)):\n",
    "        os.makedirs(join(results_dir, output_directory))\n",
    "    output = join(output_directory, 'rep_seqs_tax_assignments.qza')\n",
    "    cmd.extend(['--o-classification', '\"' + output + '\"'])\n",
    "    \n",
    "    return output, ' '.join(cmd)\n",
    "    \n",
    "def get_combinations(params):\n",
    "    params, values = zip(*params.items())\n",
    "    for combination in product(*values):\n",
    "        yield dict(zip(params, combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(join(results_dir, 'classifiers')):\n",
    "    os.makedirs(join(results_dir, 'classifiers'))\n",
    "\n",
    "classifier_commands = set()\n",
    "classify_commands = []\n",
    "classifiers = set()\n",
    "classifications = []\n",
    "for mock in mock_dirs:\n",
    "    reads = join('..', 'data', 'mock-community', mock, 'rep_seqs.qza')\n",
    "    mock_directory = join('classifications', mock)\n",
    "    inputs = {'reference-taxonomy': taxs[mock]}\n",
    "    for db_name, db_file in ref_dbs[mock]:\n",
    "        db_directory = join(mock_directory, db_name)\n",
    "        inputs['reference-reads'] = db_file\n",
    "        for method in classifier_params:\n",
    "            method_directory = join(db_directory, method)\n",
    "            for params in get_combinations(classifier_params[method]):\n",
    "                priors = priors_files[mock][db_name]\n",
    "                classifier, command = get_classifier_command(method, inputs, params, priors)\n",
    "                classifier_commands.add(command)\n",
    "                classifiers.add(classifier)\n",
    "                for confidence in confidences:\n",
    "                    classification, command = get_classify_command(\n",
    "                        classifier, reads, params, confidence,\n",
    "                        method_directory, results_dir)\n",
    "                    classifications.append(classification)\n",
    "                    classify_commands.append(command)\n",
    "                \n",
    "# write out the commands\n",
    "with open(join(results_dir, 'classifier_commands.sh'), 'w') as cmds:\n",
    "    for cmd in classifier_commands:\n",
    "        cmds.write(cmd + '\\n')\n",
    "with open(join(results_dir, 'classify_commands.sh'), 'w') as cmds:\n",
    "    for cmd in classify_commands:\n",
    "        cmds.write(cmd + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Additional files for tax-credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_classifications = []\n",
    "for classification in classifications:\n",
    "    full_classification = join(results_dir, classification)\n",
    "    output_dir = os.path.dirname(full_classification)\n",
    "\n",
    "    taxonomy_map_fp = join(output_dir, 'taxonomy.tsv')      \n",
    "    if not os.path.exists(taxonomy_map_fp):\n",
    "        try:\n",
    "            Artifact.load(full_classification).export_data(output_dir)\n",
    "        except ValueError:\n",
    "            bad_classifications.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad classifications\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(len(bad_classifications), \"bad classifications\")\n",
    "bc_combinations = None\n",
    "for bc in bad_classifications:\n",
    "    if '[4,16]' not in bc:\n",
    "        print(bc)\n",
    "        continue\n",
    "    sbc = []\n",
    "    for tbc in bc.split(os.path.sep):\n",
    "        sbc.extend(tbc.split(':'))\n",
    "    if bc_combinations is None:\n",
    "        bc_combinations = [{tbc} for tbc in sbc]\n",
    "    else:\n",
    "        for tbc, bcc in zip(sbc, bc_combinations):\n",
    "            bcc.add(tbc)\n",
    "print(bc_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxonomy_glob = join(results_dir, 'classifications', 'mock-*', '*', 'naive-bayes*', '*', 'taxonomy.tsv')\n",
    "generate_per_method_biom_tables(taxonomy_glob, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precomputed_results_dir = join(project_dir, \"data\", \"precomputed-results\", analysis_name)\n",
    "method_dirs = glob.glob(join(results_dir, 'classifications', 'mock-*', '*', 'naive-bayes*', '*'))\n",
    "move_results_to_repository(method_dirs, precomputed_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
